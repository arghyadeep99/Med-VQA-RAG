{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0db5ebf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T20:26:18.815877Z",
     "iopub.status.busy": "2025-05-11T20:26:18.815543Z",
     "iopub.status.idle": "2025-05-11T20:26:18.937873Z",
     "shell.execute_reply": "2025-05-11T20:26:18.936532Z"
    },
    "papermill": {
     "duration": 0.131313,
     "end_time": "2025-05-11T20:26:18.939896",
     "exception": false,
     "start_time": "2025-05-11T20:26:18.808583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b84c321",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T20:26:18.952200Z",
     "iopub.status.busy": "2025-05-11T20:26:18.951797Z",
     "iopub.status.idle": "2025-05-11T20:26:20.178893Z",
     "shell.execute_reply": "2025-05-11T20:26:20.177626Z"
    },
    "papermill": {
     "duration": 1.236128,
     "end_time": "2025-05-11T20:26:20.181375",
     "exception": false,
     "start_time": "2025-05-11T20:26:18.945247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "The token `med-alpaca-685` has been saved to /root/.cache/huggingface/stored_tokens\r\n",
      "Your token has been saved to /root/.cache/huggingface/token\r\n",
      "Login successful.\r\n",
      "The current active token is: `med-alpaca-685`\r\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369cc88e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T20:26:20.193803Z",
     "iopub.status.busy": "2025-05-11T20:26:20.193437Z",
     "iopub.status.idle": "2025-05-11T20:28:03.188491Z",
     "shell.execute_reply": "2025-05-11T20:28:03.187198Z"
    },
    "papermill": {
     "duration": 103.003867,
     "end_time": "2025-05-11T20:28:03.190414",
     "exception": false,
     "start_time": "2025-05-11T20:26:20.186547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\r\n",
      "Collecting transformers==4.48.0\r\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.30.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (0.5.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.48.0) (4.67.1)\r\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (2025.3.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0) (4.13.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.48.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.48.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.48.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.48.0) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.48.0) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers==4.48.0) (2.4.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.48.0) (2025.1.31)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.48.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers==4.48.0) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.48.0) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.48.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers==4.48.0) (2024.2.0)\r\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, bitsandbytes\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.51.1\r\n",
      "    Uninstalling transformers-4.51.1:\r\n",
      "      Successfully uninstalled transformers-4.51.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.48.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes accelerate transformers==4.48.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3396b92d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T20:28:03.256516Z",
     "iopub.status.busy": "2025-05-11T20:28:03.256177Z",
     "iopub.status.idle": "2025-05-11T20:28:05.674515Z",
     "shell.execute_reply": "2025-05-11T20:28:05.673135Z"
    },
    "papermill": {
     "duration": 2.45489,
     "end_time": "2025-05-11T20:28:05.676434",
     "exception": false,
     "start_time": "2025-05-11T20:28:03.221544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\r\n",
      "Version: 4.48.0\r\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\r\n",
      "Home-page: https://github.com/huggingface/transformers\r\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\r\n",
      "Author-email: transformers@huggingface.co\r\n",
      "License: Apache 2.0 License\r\n",
      "Location: /usr/local/lib/python3.11/dist-packages\r\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\r\n",
      "Required-by: kaggle-environments, peft, sentence-transformers\r\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b436cf",
   "metadata": {
    "execution": {},
    "papermill": {
     "duration": 1.03711,
     "end_time": "2025-05-11T20:28:06.746059",
     "exception": false,
     "start_time": "2025-05-11T20:28:05.708949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#restart kernel\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c1bbb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T19:16:25.653525Z",
     "iopub.status.busy": "2025-05-11T19:16:25.653239Z",
     "iopub.status.idle": "2025-05-11T19:20:14.283456Z",
     "shell.execute_reply": "2025-05-11T19:20:14.282804Z",
     "shell.execute_reply.started": "2025-05-11T19:16:25.653502Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5f12a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Normal LLama 3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aeb667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T19:21:50.936635Z",
     "iopub.status.busy": "2025-05-11T19:21:50.935861Z",
     "iopub.status.idle": "2025-05-11T19:21:51.206517Z",
     "shell.execute_reply": "2025-05-11T19:21:51.205821Z",
     "shell.execute_reply.started": "2025-05-11T19:21:50.936606Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3a6dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T23:54:07.177902Z",
     "iopub.status.busy": "2025-05-08T23:54:07.177592Z",
     "iopub.status.idle": "2025-05-08T23:54:59.936695Z",
     "shell.execute_reply": "2025-05-08T23:54:59.935862Z",
     "shell.execute_reply.started": "2025-05-08T23:54:07.177878Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# image = Image.open('/kaggle/input/test12/test_img_3.jpg')\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"image\"},\n",
    "#         {\"type\": \"text\", \"text\": \"what are the abnormalities in the image?\"}\n",
    "#     ]}\n",
    "# ]\n",
    "# input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "# inputs = processor(\n",
    "#     image,\n",
    "#     input_text,\n",
    "#     add_special_tokens=False,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# output = model.generate(**inputs, max_new_tokens=500)\n",
    "# print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb9bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T19:22:38.881784Z",
     "iopub.status.busy": "2025-05-11T19:22:38.881116Z",
     "iopub.status.idle": "2025-05-11T19:22:38.886823Z",
     "shell.execute_reply": "2025-05-11T19:22:38.886028Z",
     "shell.execute_reply.started": "2025-05-11T19:22:38.881755Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0d0ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T19:35:14.139063Z",
     "iopub.status.busy": "2025-05-11T19:35:14.138415Z",
     "iopub.status.idle": "2025-05-11T19:35:14.222850Z",
     "shell.execute_reply": "2025-05-11T19:35:14.222238Z",
     "shell.execute_reply.started": "2025-05-11T19:35:14.139038Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# vqa_rad_dir = \"/kaggle/input/vqa-rad/VQA-RAD-Chest/VQA_RAD_Chest_Image_Folder\"\n",
    "# df = pd.read_csv('/kaggle/input/vqa-rad/VQA-RAD-Chest/VQA_RAD_Chest_Data.csv')\n",
    "\n",
    "slake_dir = \"/kaggle/input/slake-chest-images/SLAKE_Chest\"\n",
    "df = pd.read_csv(\"/kaggle/input/slake-chest-images/SLAKE_Chest/Slake_Chest_Data_Results_Both.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609c5f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T20:09:19.223998Z",
     "iopub.status.busy": "2025-05-11T20:09:19.223228Z",
     "iopub.status.idle": "2025-05-11T20:09:19.227809Z",
     "shell.execute_reply": "2025-05-11T20:09:19.227176Z",
     "shell.execute_reply.started": "2025-05-11T20:09:19.223972Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df=df[df.answer_type==\"OPEN\"]\n",
    "print(len(df))\n",
    "CSV_OUTPUT_FILE = \"/kaggle/working/LLama_3_2_VI_Baseline_SLAKE.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e2aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T20:09:34.742326Z",
     "iopub.status.busy": "2025-05-11T20:09:34.741678Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "responses = []\n",
    "\n",
    "output_file_exists = os.path.exists(CSV_OUTPUT_FILE)\n",
    "\n",
    "with open(CSV_OUTPUT_FILE, mode=\"a+\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"qid\", \"img_name\", \"question\", \"generated_response\"])\n",
    "\n",
    "    # if not output_file_exists:\n",
    "    writer.writeheader()\n",
    "        \n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"Processing row {idx + 1}/{len(df)}\")\n",
    "        \n",
    "        image_id = row['img_name'].split('/', maxsplit=1)[1]\n",
    "        image_file = os.path.join(slake_dir, image_id)\n",
    "    \n",
    "        image = load_image(image_file)\n",
    "        prompt=f\"You are an expert radiologist. Considering the given image, answer the following question in a single short paragraph. Do not use bullet points. Be precise in your responses. Question : {row['question']}\"\n",
    "        messages = [\n",
    "                {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "                ]}\n",
    "            ]\n",
    "        \n",
    "        input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        input_token_len = inputs[\"input_ids\"].size(1)\n",
    "    \n",
    "        output = model.generate(**inputs, max_new_tokens=200)\n",
    "        gen_ids = output[0, input_token_len:]   # shape: (new_token_count,)\n",
    "        response = processor.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        content_to_write = {\n",
    "            \"qid\": row[\"qid\"],\n",
    "            \"img_name\": image_id,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"generated_response\": response\n",
    "        }\n",
    "        writer.writerow(content_to_write)\n",
    "        # # response=processor.decode(output[0])[:-10]\n",
    "        # responses.append(response)\n",
    "\n",
    "        print(f\"Generated response for row {idx + 1}/{len(df)} and written to CSV.\")\n",
    "        \n",
    "        if idx%10 == 0:\n",
    "            print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc65eec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2083d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T16:29:10.998781Z",
     "iopub.status.busy": "2025-05-11T16:29:10.998609Z",
     "iopub.status.idle": "2025-05-11T16:29:11.045728Z",
     "shell.execute_reply": "2025-05-11T16:29:11.045034Z",
     "shell.execute_reply.started": "2025-05-11T16:29:10.998768Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['llama_3_2-VI response'] = responses\n",
    "df.to_csv('/kaggle/working/Vanilla_VQA_RAD_LLama_3_2.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980e25f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Llava-rad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0562fe2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced4479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:16:37.014864Z",
     "iopub.status.busy": "2025-05-11T01:16:37.014230Z",
     "iopub.status.idle": "2025-05-11T01:16:37.655013Z",
     "shell.execute_reply": "2025-05-11T01:16:37.654170Z",
     "shell.execute_reply.started": "2025-05-11T01:16:37.014833Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/microsoft/llava-rad.git\n",
    "%cd llava-rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f031da02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:17:05.599424Z",
     "iopub.status.busy": "2025-05-11T01:17:05.598798Z",
     "iopub.status.idle": "2025-05-11T01:20:05.083040Z",
     "shell.execute_reply": "2025-05-11T01:20:05.082099Z",
     "shell.execute_reply.started": "2025-05-11T01:17:05.599398Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eef655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:21:09.440373Z",
     "iopub.status.busy": "2025-05-11T01:21:09.440008Z",
     "iopub.status.idle": "2025-05-11T01:21:11.506206Z",
     "shell.execute_reply": "2025-05-11T01:21:11.505416Z",
     "shell.execute_reply.started": "2025-05-11T01:21:09.440345Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f197b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:21:14.959064Z",
     "iopub.status.busy": "2025-05-11T01:21:14.958795Z",
     "iopub.status.idle": "2025-05-11T01:21:34.751568Z",
     "shell.execute_reply": "2025-05-11T01:21:34.750832Z",
     "shell.execute_reply.started": "2025-05-11T01:21:14.959041Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "# import pandas as pd\n",
    "import os\n",
    "%cd llava-rad\n",
    "from llava.constants import IMAGE_TOKEN_INDEX\n",
    "from llava.conversation import conv_templates\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2808f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:21:46.890127Z",
     "iopub.status.busy": "2025-05-11T01:21:46.889361Z",
     "iopub.status.idle": "2025-05-11T01:21:46.894216Z",
     "shell.execute_reply": "2025-05-11T01:21:46.893485Z",
     "shell.execute_reply.started": "2025-05-11T01:21:46.890101Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(image_file):\n",
    "    if image_file.startswith('http') or image_file.startswith('https'):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    else:\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa342a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:21:59.622131Z",
     "iopub.status.busy": "2025-05-11T01:21:59.621517Z",
     "iopub.status.idle": "2025-05-11T01:26:17.471587Z",
     "shell.execute_reply": "2025-05-11T01:26:17.470962Z",
     "shell.execute_reply.started": "2025-05-11T01:21:59.622103Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "disable_torch_init()\n",
    "\n",
    "model_path = \"microsoft/llava-rad\"\n",
    "model_base = \"lmsys/vicuna-7b-v1.5\"\n",
    "model_name = \"llavarad\"\n",
    "conv_mode = \"v1\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fda1ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:27:11.511846Z",
     "iopub.status.busy": "2025-05-11T01:27:11.511306Z",
     "iopub.status.idle": "2025-05-11T01:27:11.565734Z",
     "shell.execute_reply": "2025-05-11T01:27:11.565119Z",
     "shell.execute_reply.started": "2025-05-11T01:27:11.511820Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "vqa_rad_dir = \"/kaggle/input/vqa-rad/VQA-RAD-Chest/VQA_RAD_Chest_Image_Folder\"\n",
    "df = pd.read_csv('/kaggle/input/vqa-rad/VQA-RAD-Chest/VQA_RAD_Chest_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c443a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T04:13:30.460139Z",
     "iopub.status.busy": "2025-05-11T04:13:30.459520Z",
     "iopub.status.idle": "2025-05-11T04:13:30.478991Z",
     "shell.execute_reply": "2025-05-11T04:13:30.478325Z",
     "shell.execute_reply.started": "2025-05-11T04:13:30.460114Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df[df.A_TYPE==\"OPEN\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf23cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T01:54:19.790559Z",
     "iopub.status.busy": "2025-05-11T01:54:19.789927Z",
     "iopub.status.idle": "2025-05-11T02:21:32.730893Z",
     "shell.execute_reply": "2025-05-11T02:21:32.729718Z",
     "shell.execute_reply.started": "2025-05-11T01:54:19.790534Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "  image_id = row['IMAGEID'].split('/')[-1]\n",
    "  image_file = os.path.join(vqa_rad_dir, image_id)\n",
    "  query = f\"<image>{row['QUESTION']}\"\n",
    "  conv = conv_templates[conv_mode].copy()\n",
    "  conv.append_message(conv.roles[0], query)\n",
    "  conv.append_message(conv.roles[1], None)\n",
    "  prompt = conv.get_prompt()\n",
    "\n",
    "  print(prompt)\n",
    "  image = load_image(image_file)\n",
    "  image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0].half().unsqueeze(0).cuda()\n",
    "\n",
    "  input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "  stopping_criteria = KeywordsStoppingCriteria([\"</s>\"], tokenizer, input_ids)\n",
    "\n",
    "  with torch.inference_mode():\n",
    "      output_ids = model.generate(\n",
    "          input_ids,\n",
    "          images=image_tensor,\n",
    "          do_sample=False,\n",
    "          temperature=0.0,\n",
    "          max_new_tokens=1024,\n",
    "          use_cache=True)\n",
    "\n",
    "  outputs = tokenizer.batch_decode(output_ids[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "  outputs = outputs.strip()\n",
    "  responses.append(outputs)\n",
    "  print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b18d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T02:21:32.825725Z",
     "iopub.status.busy": "2025-05-11T02:21:32.825529Z",
     "iopub.status.idle": "2025-05-11T02:21:32.847097Z",
     "shell.execute_reply": "2025-05-11T02:21:32.846399Z",
     "shell.execute_reply.started": "2025-05-11T02:21:32.825702Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['llava-rad response'] = responses\n",
    "df.to_csv('/kaggle/working/Vanilla_VQA_RAD_LLaVA-RAD.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e099724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T02:23:58.171376Z",
     "iopub.status.busy": "2025-05-11T02:23:58.170998Z",
     "iopub.status.idle": "2025-05-11T02:23:58.181317Z",
     "shell.execute_reply": "2025-05-11T02:23:58.180713Z",
     "shell.execute_reply.started": "2025-05-11T02:23:58.171348Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8a69e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7391863,
     "sourceId": 11773732,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 113.123198,
   "end_time": "2025-05-11T20:28:06.794056",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-11T20:26:13.670858",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
