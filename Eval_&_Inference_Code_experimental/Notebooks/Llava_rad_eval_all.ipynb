{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iqMWgXzCtUl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/microsoft/llava-rad.git\n",
        "%cd llava-rad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "YvXzDVgCCya2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "# import pandas as pd\n",
        "import os\n",
        "%cd llava-rad\n",
        "from llava.constants import IMAGE_TOKEN_INDEX\n",
        "from llava.conversation import conv_templates\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.utils import disable_torch_init\n",
        "from llava.mm_utils import tokenizer_image_token, KeywordsStoppingCriteria"
      ],
      "metadata": {
        "id": "W3JJpTQTCz8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_file):\n",
        "    if image_file.startswith('http') or image_file.startswith('https'):\n",
        "        response = requests.get(image_file)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(image_file).convert('RGB')\n",
        "    return image"
      ],
      "metadata": {
        "id": "1mYX_t-_C5Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disable_torch_init()\n",
        "\n",
        "model_path = \"microsoft/llava-rad\"\n",
        "model_base = \"lmsys/vicuna-7b-v1.5\"\n",
        "model_name = \"llavarad\"\n",
        "conv_mode = \"v1\"\n",
        "\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name)"
      ],
      "metadata": {
        "id": "hvU4EZPjC665"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "vqa_rad_dir = \"/kaggle/input/vqa-rad/VQA-RAD-Chest/VQA_RAD_Chest_Image_Folder\"\n",
        "df = pd.read_csv('/kaggle/input/vqa-rad/VQA-RAD-Chest/VQA_RAD_Chest_Data.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "c_QRL28dC9CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[df.A_TYPE==\"OPEN\"]\n",
        "df"
      ],
      "metadata": {
        "id": "O-LAhk2qC_zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  image_id = row['IMAGEID'].split('/')[-1]\n",
        "  image_file = os.path.join(vqa_rad_dir, image_id)\n",
        "  query = f\"<image>{row['QUESTION']}\"\n",
        "  conv = conv_templates[conv_mode].copy()\n",
        "  conv.append_message(conv.roles[0], query)\n",
        "  conv.append_message(conv.roles[1], None)\n",
        "  prompt = conv.get_prompt()\n",
        "\n",
        "  print(prompt)\n",
        "  image = load_image(image_file)\n",
        "  image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0].half().unsqueeze(0).cuda()\n",
        "\n",
        "  input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
        "\n",
        "  stopping_criteria = KeywordsStoppingCriteria([\"</s>\"], tokenizer, input_ids)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "      output_ids = model.generate(\n",
        "          input_ids,\n",
        "          images=image_tensor,\n",
        "          do_sample=False,\n",
        "          temperature=0.0,\n",
        "          max_new_tokens=1024,\n",
        "          use_cache=True)\n",
        "\n",
        "  outputs = tokenizer.batch_decode(output_ids[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
        "  outputs = outputs.strip()\n",
        "  responses.append(outputs)\n",
        "  print(i+1)"
      ],
      "metadata": {
        "id": "8zJV8fmEDBgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['llava-rad response'] = responses\n",
        "df.to_csv('/kaggle/working/Vanilla_VQA_RAD_LLaVA-RAD.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "o1wf9wLxDCHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses"
      ],
      "metadata": {
        "id": "kePQOM6GDKEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}